---
title: 'Machine Learning for differential equations? Is that a thing?'
date: 2023-11-03
permalink: /posts/2023/11/03/
tags:
  - machine_learning
---

Once in a while I hear the question how machine learning can have any connection with differential equations?

![fast_with_math](https://mnaderibeni.github.io/images/ML4DEs.png)

An interesting example that I can mention is the diffusion model (in other words: Score-Based Generative Modeling with Stochastic Differential Equations). These models were introduced not a very long ago with several prominent research papers titled: 
1. [Deep Unsupervised Learning using Nonequilibrium Thermodynamics](https://arxiv.org/abs/1503.03585)

2. [Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239)

3. [Score-Based Generative Modeling through Stochastic Differential Equations](https://arxiv.org/abs/2011.13456)

and they have played prominent role in enabling high quality/resolution generative modeling.

To put in very simple words, generative modeling with diffusion models is like the gradient ascent method; to have an initial guess and to move it towards the maximum of the function (in this case the probability density function).

In this method rather than learning the probability distribution, the gradient of its logarithm is being learned (score-function). The probability density function is an scalar and its gradient is a vector field. For a point in feature space we don't learn the value of the scaler but we try to learn the components of a vector field that point towards the maximum regions. With this vector field being learned, we can sample any random point in the feature space and move it along this vector field towards the hig probable regions.


To train a generative model, we start with samples that already belong to the high probable regions of the probability distribution. Therefore it is not very difficult to learn the vector field in these regions. However such regions cover a very small portion of the feature space and it is rather impossible to learn the score-function (the vector field) in regions with no samples. This was the main pitfall of score-based generative models until this problem was solved with diffusion models. To put it in simple words, the solution is to populate the feature space with noisy training data. This is achieved by adding multiple scales of noise. This is helpful in learning a close approximate of the mentioned vector filed. 

In sample generation mode, we usually start with pure noise and move it along the learned score-function. This procidue is an iterative numerical approach to solve a stochastic differential equation.

Some usfull links:

- [Yang Song's blog post](https://yang-song.net/blog/2021/score/)
- [Diffusion and Score-Based Generative Models](https://www.youtube.com/watch?v=wMmqCMwuM2Q&ab_channel=MITCBMM)
- [Advancements in Diffusion Models for Generative AI](https://www.youtube.com/watch?v=y8q3gh61OY0&ab_channel=YouthAILab)



